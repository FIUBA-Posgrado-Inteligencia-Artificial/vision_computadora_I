{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Stiching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Version de OpenCV: 3.4.2\n"
     ]
    }
   ],
   "source": [
    "#Si queremos que las imágenes sean mostradas en una ventana emergente quitar el inline\n",
    "# %matplotlib inline\n",
    "%matplotlib \n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "print(f'Version de OpenCV: {cv.__version__}')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo vamos a generar una vista panoramica a partir de 2 imagenes. La unica condicion es que las imagenes a utilizar presenten un solapamiento parcial para poder encontrar puntos en comun dentro de la escena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17eeda97c08>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos la imagen a procesar\n",
    "img_left = cv.imread('imagenes/pano_1.jpg')\n",
    "img_right = cv.imread('imagenes/pano_2.jpg')\n",
    "\n",
    "# La transformamos en escala de grises\n",
    "gray_left = cv.cvtColor(img_left, cv.COLOR_BGR2GRAY)\n",
    "gray_right = cv.cvtColor(img_right, cv.COLOR_BGR2GRAY)\n",
    "img_left = cv.cvtColor(img_left, cv.COLOR_BGR2RGB)\n",
    "img_right = cv.cvtColor(img_right, cv.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img_left)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Obtenemos los descriptores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17eebe90788>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos el vector de características SIFT\n",
    "sift = cv.xfeatures2d.SIFT_create()\n",
    "\n",
    "# Y buscamos según el algoritmo...\n",
    "kps_left, descriptors_left = sift.detectAndCompute(gray_left, None)\n",
    "kps_right, descriptors_right = sift.detectAndCompute(gray_right, None)\n",
    "\n",
    "# me quedo solo con valores x,y de la posicion de los descriptores\n",
    "kps_loc_left = np.float32([kp.pt for kp in kps_left])\n",
    "kps_loc_right = np.float32([kp.pt for kp in kps_right])\n",
    "\n",
    "# BFMatcher con parámetros por defecto\n",
    "bf = cv.BFMatcher(cv.NORM_L2, crossCheck=True)\n",
    "matches = bf.match(descriptors_left, descriptors_right)\n",
    "\n",
    "# Los ordenamos según distancia\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "# Dibujamos las primeras 50 coincidencias\n",
    "n=50\n",
    "img_matched = cv.drawMatches(img_left, kps_left, img_right, kps_right, matches[:n], None, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(matches) > 4:\n",
    "    # separo las coordenadas de los keypoints que fueron matcheados\n",
    "    ptsA = np.float32([kps_loc_left[m.queryIdx] for m in matches])\n",
    "    ptsB = np.float32([kps_loc_right[m.trainIdx] for m in matches])\n",
    "    \n",
    "    # Calculamos la matriz de homografia\n",
    "    (H, status) = cv.findHomography(ptsB, ptsA, cv.RANSAC, 4.0)\n",
    "    \n",
    "    result = cv.warpPerspective(img_right, H,(img_left.shape[1] + img_right.shape[1], img_right.shape[0]))\n",
    "    result[0:img_left.shape[0], 0:img_right.shape[1]] = img_left\n",
    "    # mostramos la imagen\n",
    "    plt.figure()\n",
    "    plt.imshow(result)\n",
    "    # plt.imshow(result[100:-100,:-100,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metemos todo en una funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_stiching(img_left, img_right) -> np.ndarray:\n",
    "    '''\n",
    "    Image stiching between 2 BGR frames using SIFT descriptor and brute force\n",
    "    keypoints matching. \n",
    "\n",
    "    Receive\n",
    "    ------------------------\n",
    "    img_left:  BGR image, numpy ndarray like\n",
    "    img_right:  BGR image, numpy ndarray like\n",
    "\n",
    "    Returns\n",
    "    -------------------------\n",
    "    result: BGR image, numpy ndarray like or None if no matches were found\n",
    "    '''\n",
    "    gray_left = cv.cvtColor(img_left, cv.COLOR_BGR2GRAY)\n",
    "    gray_right = cv.cvtColor(img_right, cv.COLOR_BGR2GRAY)\n",
    "    \n",
    "    sift = cv.xfeatures2d.SIFT_create()\n",
    "    kps_left, descriptors_left = sift.detectAndCompute(gray_left, None)\n",
    "    kps_right, descriptors_right = sift.detectAndCompute(gray_right, None)\n",
    "\n",
    "    kps_loc_left = np.float32([kp.pt for kp in kps_left])\n",
    "    kps_loc_right = np.float32([kp.pt for kp in kps_right])\n",
    "    bf = cv.BFMatcher(cv.NORM_L2, crossCheck=True)\n",
    "    matches = bf.match(descriptors_left, descriptors_right)\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "    if len(matches) > 4:\n",
    "\n",
    "        ptsA = np.float32([kps_loc_left[m.queryIdx] for m in matches])\n",
    "        ptsB = np.float32([kps_loc_right[m.trainIdx] for m in matches])\n",
    "\n",
    "        (H, status) = cv.findHomography(ptsB, ptsA, cv.RANSAC, 4.0)\n",
    "\n",
    "        result = cv.warpPerspective(img_right, H,(img_left.shape[1] + img_right.shape[1], img_right.shape[0]))\n",
    "        result[0:img_left.shape[0], 0:img_right.shape[1]] = img_left\n",
    "        return result\n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('vision-robotica')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0af437b5139b375ee3fab2b21e8a1376590e14b3c752e237587dc080bd8d5be3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
